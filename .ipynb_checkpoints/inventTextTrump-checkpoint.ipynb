{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "import spacy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "discourse = 'Trump'\n",
    "# There are four options for vector spaces of words, which represent\n",
    "# different discourses, or the ways in which language is used: Trump,\n",
    "# Balzac, Sand, Flaubert.\n",
    "# See below.\n",
    "\n",
    "number_of_options = 15\n",
    "# the max number of similar words proposed from the vector space\n",
    "# for each word in the asserted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = u'strong'\n",
    "negative = u'weak'\n",
    "# These two words establish the analogy for finding similar words in\n",
    "# the vector space.\n",
    "\n",
    "assertion = u\"There are few issues more important to the security \" + \\\n",
    "            u\"of the US than the potential spread of nuclear \" + \\\n",
    "            u\"weapons or the potential for even more destructive \" + \\\n",
    "            u\"war in the Middle East. Today’s decision to put the \" + \\\n",
    "            u\"JCPOA at risk is a serious mistake.\"\n",
    "# This assertion, a tweet by Barack Obama on 8 May 2018, will be altered by word\n",
    "# substitutions based on the above analogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'Trump':\n",
    "        ['Trump_model',\n",
    "         # a vector space of words from all of Trump's tweets\n",
    "         \n",
    "         'Trump_pos_dict.pkl',\n",
    "         # a dictionary of all words in the vector space with part-of-speech (POS) tags\n",
    "         \n",
    "         'en',\n",
    "         # the language of the vector space\n",
    "         \n",
    "         ('DT', 'PUNCT', 'IN')\n",
    "         # POS tags for words that will not be replaced in asserted text\n",
    "         \n",
    "        ],\n",
    "    'Balzac':\n",
    "        ['NCF_short_author_Balzac_model',\n",
    "         # vector space of words from 118 volumes by Balzac\n",
    "         \n",
    "         'NCF_pos_dict.pkl',\n",
    "         # this dictionary is based on a corpus of 1,333 volumes of 19C French texts\n",
    "         \n",
    "         'fr',\n",
    "         ('DET', 'PUNCT')\n",
    "        ],\n",
    "    'Sand':\n",
    "        ['NCF_short_author_Sand_model',\n",
    "         # vector space of words from 70 volumes by Sand\n",
    "         \n",
    "         'NCF_pos_dict.pkl',\n",
    "         'fr',\n",
    "         ('DET', 'PUNCT')\n",
    "        ],\n",
    "    'Flaubert':\n",
    "        ['NCF_short_author_Flaubert_model',\n",
    "         # vector space of words from 30 volumes by Flaubert\n",
    "         \n",
    "         'NCF_pos_dict.pkl',\n",
    "         'fr',\n",
    "         ('DET', 'PUNCT')\n",
    "        ]\n",
    "}\n",
    "\n",
    "model = gensim.models.Word2Vec.load(params[discourse][0])\n",
    "pickleFile = open(params[discourse][1], 'rb')\n",
    "posd = pickle.load(pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(params[discourse][2])\n",
    "parsed = nlp(assertion)\n",
    "words = [(w.text.lower(), w.tag_, w.lemma_.lower()) for w in parsed]\n",
    "# Build a list of 3-tuples for each word in the asserted text:\n",
    "# (the word in the asserted text, its POS, its lemma)\n",
    "\n",
    "new_words = []\n",
    "\n",
    "for word in words:\n",
    "    try:\n",
    "        hits = []\n",
    "        # a list of vector space words to be built that will be similar to a word\n",
    "        # in the asserted text.\n",
    "        \n",
    "        psw = word[1].split('__')[0]\n",
    "        # The POS tag for a word in the asserted text.\n",
    "        \n",
    "        #print word[0], word[1], word[2] # for debugging\n",
    "        \n",
    "        for item in model.wv.most_similar(positive=[positive.lower(), word[2]],\n",
    "                                          negative=[negative.lower()],\n",
    "                                          topn=number_of_options):\n",
    "        # Take each word in the asserted text and look for similar words\n",
    "        # in the vector space based on the analogy.\n",
    "        \n",
    "            #print '\\t', item # for debugging\n",
    "            \n",
    "            if posd[item[0]]:\n",
    "            # does the vector-space word have a POS tag?\n",
    "            \n",
    "                psd = next(iter(posd[item[0]])).split('__')[0]\n",
    "                \n",
    "                #print '\\t\\t', psd # for debugging\n",
    "                \n",
    "                if (psw not in params[discourse][3]) and (psw == psd):\n",
    "                # We exclude certain POS words (like determiners and punctuation: see above)\n",
    "                # to maintain readability in the invented text.\n",
    "                # We also select words from the vector space that are the same POS\n",
    "                # as the original word in the asserted text.\n",
    "                \n",
    "                    hits.append(item[0])\n",
    "                    \n",
    "        if len(hits) > 0:\n",
    "        # Did we find at least one vector space word with the same POS?\n",
    "        # If so, display them in parentheses in the invented text.\n",
    "        \n",
    "            replacement = '(' + '|'.join(hits) + ')'\n",
    "            new_words.append(replacement)\n",
    "            \n",
    "        else:\n",
    "        # If we found nothing that matches, use the original word.\n",
    "        \n",
    "            new_words.append(word[0])\n",
    "    except:\n",
    "    # If something weird happens, just use the original word.\n",
    "    \n",
    "        new_words.append(word[0])\n",
    "        \n",
    "        #print 'EXCEPTION', word[0] # for debugging\n",
    "\n",
    "response = ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are few issues more important to the security of the US than the potential spread of nuclear weapons or the potential for even more destructive war in the Middle East. Today’s decision to put the JCPOA at risk is a serious mistake. \n",
      "\n",
      "there (tell) (low) issues more (beautiful|low|happy) to the security of the (gop|maga|state) than the (low) spread of (low) (congratulation) or the potential for (then) more (exceptional) war in the (w|gop) (sunday|hotel|university) . (tomorrow|poll) ’s (gift) to put the jcpoa at (chairman|class|food) is a (important|low) (job|friend|hope) .\n"
     ]
    }
   ],
   "source": [
    "print assertion, '\\n'\n",
    "print response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'move', 0.9992275238037109), (u'important', 0.9991986155509949), (u'tax', 0.9991898536682129), (u'yet', 0.9991799592971802), (u'pay', 0.9991798400878906), (u'bill', 0.9991753101348877), (u'us', 0.9991708993911743), (u'low', 0.9991667866706848), (u'other', 0.9991663694381714), (u'justice', 0.9991630911827087), (u'cut', 0.9991627335548401), (u'release', 0.999162495136261), (u'put', 0.9991592168807983), (u'china', 0.9991582632064819), (u'major', 0.9991567730903625)]\n"
     ]
    }
   ],
   "source": [
    "print model.wv.most_similar(positive=[positive, u'serious'],\n",
    "                            negative=[negative], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'now', 0.9994233250617981), (u'to', 0.9994038939476013), (u'fight', 0.999396800994873), (u'proud', 0.9993846416473389), (u'job', 0.9993821978569031), (u'start', 0.9993764758110046), (u'tough', 0.9993470907211304), (u'will', 0.9993371963500977), (u'deal', 0.9993306994438171), (u'a', 0.9993232488632202), (u'once', 0.9993203282356262), (u'friend', 0.9993187785148621), (u'man', 0.999317467212677), (u'hope', 0.9993098974227905), (u'order', 0.99930739402771)]\n"
     ]
    }
   ],
   "source": [
    "print model.wv.most_similar(positive=[positive, u'mistake'],\n",
    "                            negative=[negative], topn=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
