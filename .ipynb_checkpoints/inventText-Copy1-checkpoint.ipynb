{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorithmic Invention\n",
    "### Mark Wolff<br>Hartwick College\n",
    "#### ELO 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\>\\>\\> model.wv.most_similar(positive=['femme', 'roi'], negative=['homme'], topn=1)\n",
    "\n",
    "[(u'reine', 0.8085041046142578)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"2100\"\n",
       "            height=\"800\"\n",
       "            src=\"http://www.ghostweather.com/files/word2vecpride/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10f10ba50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('http://www.ghostweather.com/files/word2vecpride/', 2100, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![alt text](NCF_short_author_Flaubert_tsne_plot.svg \"Word Vector Model for Flaubert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import gensim\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "discourse = 'Flaubert'\n",
    "# There are four options for vector spaces of words, which represent\n",
    "# different discourses, or the ways in which language is used: Trump,\n",
    "# Balzac, Sand, Flaubert.\n",
    "# See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "assertion = u\"Il faut être toujours ivre. Tout est là : \" + \\\n",
    "    u\"c'est l'unique question. Pour ne pas sentir l'horrible \" + \\\n",
    "    u\"fardeau du Temps qui brise vos épaules et vous penche \" + \\\n",
    "    u\"vers la terre, il faut vous enivrer sans trêve.\"\n",
    "# The assertion, from Baudelaire's poem Enivrez-vous!, will be altered\n",
    "# by word substitutions based on the analogy below.\n",
    "\n",
    "positive = u'bien'\n",
    "negative = u'mal'\n",
    "# These two words establish the analogy for finding similar words in\n",
    "# the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'Flaubert':\n",
    "        ['NCF_Flaubert_model',\n",
    "         # vector space of words from 30 volumes by Flaubert\n",
    "         \n",
    "         'NCF_pos_dict.pkl',\n",
    "        # a dictionary of all words in the vector space with\n",
    "         # part-of-speech (POS) tags\n",
    "\n",
    "         'fr',\n",
    "         # the language of the vector space\n",
    "         \n",
    "         ('DET', 'PUNCT')\n",
    "         # POS tags for words that will not be replaced in asserted\n",
    "         # text\n",
    "        ],\n",
    "    'RussianTrolls':\n",
    "        ['RussianTrolls_model',\n",
    "         # a vector space of words from the Russian Troll tweets\n",
    "         # shared by fivethirtyeight\n",
    "         \n",
    "         'RussianTrolls_pos_dict.pkl',\n",
    "         \n",
    "         'en',\n",
    "         \n",
    "         ('DT', 'PUNCT', 'IN')\n",
    "        ]\n",
    "}\n",
    "\n",
    "number_of_options = 15\n",
    "# the max number of similar words proposed from the vector space\n",
    "# for each word in the asserted text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'NCF_short_author_Flaubert_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dee9f1578077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiscourse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpickleFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiscourse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mposd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickleFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiscourse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mark/anaconda3/envs/py27/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m         \"\"\"\n\u001b[1;32m    974\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model saved using code from earlier Gensim Version. Re-loading old model in a compatible way.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mark/anaconda3/envs/py27/lib/python2.7/site-packages/gensim/models/base_any2vec.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWordEmbeddingsModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnegative\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'index2word'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_cum_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# rebuild cum_table from vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mark/anaconda3/envs/py27/lib/python2.7/site-packages/gensim/models/base_any2vec.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mark/anaconda3/envs/py27/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mark/anaconda3/envs/py27/lib/python2.7/site-packages/gensim/utils.pyc\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \"\"\"\n\u001b[0;32m-> 1329\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mark/anaconda3/envs/py27/lib/python2.7/site-packages/smart_open/smart_open_lib.pyc\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEFAULT_ERRORS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"s3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s3n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m's3u'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms3_open_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mark/anaconda3/envs/py27/lib/python2.7/site-packages/smart_open/smart_open_lib.pyc\u001b[0m in \u001b[0;36mfile_smart_open\u001b[0;34m(fname, mode, encoding, errors)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mraw_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m     \u001b[0mraw_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0mdecompressed_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0mdecoded_fobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecompressed_fobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'NCF_short_author_Flaubert_model'"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec.load(params[discourse][0])\n",
    "pickleFile = open(params[discourse][1], 'rb')\n",
    "posd = pickle.load(pickleFile)\n",
    "\n",
    "nlp = spacy.load(params[discourse][2])\n",
    "parsed = nlp(assertion)\n",
    "words = [(w.text.lower(), w.tag_, w.lemma_.lower()) for w in parsed]\n",
    "# Build a list of 3-tuples for each word in the asserted text:\n",
    "# (the word in the asserted text, its POS, its lemma)\n",
    "\n",
    "new_words = []\n",
    "\n",
    "for word in words:\n",
    "    try:\n",
    "        hits = []\n",
    "        # a list of vector space words to be built that will be similar\n",
    "        # to a word in the asserted text.\n",
    "        \n",
    "        psw = word[1].split('__')[0]\n",
    "        # The POS tag for a word in the asserted text.\n",
    "        \n",
    "        #print word[0], word[1], word[2] # for debugging\n",
    "        \n",
    "        for item in model.wv.most_similar(positive=[positive.lower(),\n",
    "                                                    word[2]],\n",
    "                                          negative=[negative.lower()],\n",
    "                                          topn=number_of_options):\n",
    "        # Take each word in the asserted text and look for similar words\n",
    "        # in the vector space based on the analogy.\n",
    "        \n",
    "            #print '\\t', item # for debugging\n",
    "            \n",
    "            if posd[item[0]]:\n",
    "            # does the vector-space word have a POS tag?\n",
    "            \n",
    "                psd = next(iter(posd[item[0]])).split('__')[0]\n",
    "                \n",
    "                #print '\\t\\t', psd # for debugging\n",
    "                \n",
    "                if (psw not in params[discourse][3]) and (psw == psd):\n",
    "                # We exclude certain POS words (like determiners and\n",
    "                # punctuation: see above) to maintain readability in\n",
    "                # the invented text. We also select words from the\n",
    "                # vector space that are the same POS as the original\n",
    "                # word in the asserted text.\n",
    "                \n",
    "                    hits.append(item[0])\n",
    "                    \n",
    "        if len(hits) > 0:\n",
    "        # Did we find at least one vector space word with the same POS?\n",
    "        # If so, display them in parentheses in the invented text.\n",
    "        \n",
    "            replacement = '(' + '|'.join(hits) + ')'\n",
    "            new_words.append(replacement)\n",
    "            \n",
    "        else:\n",
    "        # If we found nothing that matches, use the original word.\n",
    "        \n",
    "            new_words.append(word[0])\n",
    "    except:\n",
    "    # If something weird happens, just use the original word.\n",
    "    \n",
    "        new_words.append(word[0])\n",
    "        \n",
    "        #print 'EXCEPTION', word[0] # for debugging\n",
    "\n",
    "response = ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print assertion, '\\n'\n",
    "print response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On espère être complètement fougueux. L’impossible est là : c’est l’unique œuvre. Pour davantage (peut-être) exprimer l’inspiration tyrannique du moment qui roule vos épaules et vous penche vers le banc, on espère vous effaroucher sans trêve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\">\n",
    "<p lang=\"en\" dir=\"ltr\">&quot;No one is born hating another person because of the\n",
    "color of his skin or his background or his religion...&quot;\n",
    "<a href=\"https://t.co/InZ58zkoAm\">pic.twitter.com/InZ58zkoAm</a>\n",
    "</p>&mdash; Barack Obama (@BarackObama)\n",
    "<a href=\"https://twitter.com/BarackObama/status/896523232098078720?ref_src=twsrc%5Etfw\">August 13, 2017</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-lang=\"en\">\n",
    "<p lang=\"en\" dir=\"ltr\">&quot;People must learn to hate, and if they can learn to hate,\n",
    "they can be taught to love...&quot;</p>&mdash; Barack Obama (@BarackObama)\n",
    "<a href=\"https://twitter.com/BarackObama/status/896523304873238528?ref_src=twsrc%5Etfw\">August 13, 2017</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<blockquote class=\"twitter-tweet\" data-conversation=\"none\" data-lang=\"en\">\n",
    "<p lang=\"en\" dir=\"ltr\">&quot;...For love comes more naturally to the human heart\n",
    "than its opposite.&quot; - Nelson Mandela</p>&mdash; Barack Obama (@BarackObama)\n",
    "<a href=\"https://twitter.com/BarackObama/status/896523357272911872?ref_src=twsrc%5Etfw\">August 13, 2017</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "discourse = 'RussianTrolls'\n",
    "# There are four options for vector spaces of words, which represent\n",
    "# different discourses, or the ways in which language is used: Trump,\n",
    "# Balzac, Sand, Flaubert.\n",
    "# See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "assertion = u\"No one is born hating another person because of the \" + \\\n",
    "    u\"color of his skin or his background or his religion. People \" + \\\n",
    "    u\"must learn to hate, and if they can learn to hate, they can \" + \\\n",
    "    u\"be taught to love. For love comes more naturally to the \" + \\\n",
    "    u\"human heart than its opposite.\"\n",
    "# The assertion, a tweet by Barack Obama posted August 12, 2017\n",
    "# quoting Nelson Mandela, will be altered by word substitutions\n",
    "# based on the analogy below.\n",
    "\n",
    "positive = [u'white', u'charlottesville']\n",
    "negative = [u'minority']\n",
    "# These words establish the analogy for finding similar words in\n",
    "# the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(params[discourse][0])\n",
    "pickleFile = open(params[discourse][1], 'rb')\n",
    "posd = pickle.load(pickleFile)\n",
    "\n",
    "nlp = spacy.load(params[discourse][2])\n",
    "parsed = nlp(assertion)\n",
    "words = [(w.text.lower(), w.tag_, w.lemma_.lower()) for w in parsed]\n",
    "# Build a list of 3-tuples for each word in the asserted text:\n",
    "# (the word in the asserted text, its POS, its lemma)\n",
    "\n",
    "new_words = []\n",
    "\n",
    "for word in words:\n",
    "    try:\n",
    "        hits = []\n",
    "        # a list of vector space words to be built that will be similar to a word\n",
    "        # in the asserted text.\n",
    "        \n",
    "        psw = word[1].split('__')[0]\n",
    "        # The POS tag for a word in the asserted text.\n",
    "        \n",
    "        #print word[0], word[1], word[2] # for debugging\n",
    "        \n",
    "        for item in model.wv.most_similar(positive=positive + [word[2]],\n",
    "                                          negative=negative,\n",
    "                                          topn=number_of_options):\n",
    "        # Take each word in the asserted text and look for similar words\n",
    "        # in the vector space based on the analogy.\n",
    "        \n",
    "            #print '\\t', item # for debugging\n",
    "            \n",
    "            if posd[item[0]]:\n",
    "            # does the vector-space word have a POS tag?\n",
    "            \n",
    "                psd = next(iter(posd[item[0]])).split('__')[0]\n",
    "                \n",
    "                #print '\\t\\t', psd # for debugging\n",
    "                \n",
    "                if (psw not in params[discourse][3]) and (psw == psd):\n",
    "                # We exclude certain POS words (like determiners and punctuation: see above)\n",
    "                # to maintain readability in the invented text.\n",
    "                # We also select words from the vector space that are the same POS\n",
    "                # as the original word in the asserted text.\n",
    "                \n",
    "                    hits.append(item[0])\n",
    "                    \n",
    "        if len(hits) > 0:\n",
    "        # Did we find at least one vector space word with the same POS?\n",
    "        # If so, display them in parentheses in the invented text.\n",
    "        \n",
    "            replacement = '(' + '|'.join(hits) + ')'\n",
    "            new_words.append(replacement)\n",
    "            \n",
    "        else:\n",
    "        # If we found nothing that matches, use the original word.\n",
    "        \n",
    "            new_words.append(word[0])\n",
    "    except:\n",
    "    # If something weird happens, just use the original word.\n",
    "    \n",
    "        new_words.append(word[0])\n",
    "        \n",
    "        #print 'EXCEPTION', word[0] # for debugging\n",
    "\n",
    "response = ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print assertion, '\\n'\n",
    "print response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "IFrame('http://www.alamo.free.fr/pmwiki.php?n=Logiciels.Programmes', 2100, 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "livereveal": {
   "scroll": true,
   "theme": "simple",
   "transition": "slide"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
